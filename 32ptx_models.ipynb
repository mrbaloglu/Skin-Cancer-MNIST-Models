{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "465_Project32.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn6-NylVWrRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.image as img\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBiAEvWcYLLA",
        "colab_type": "code",
        "outputId": "e886ba01-6404-4ff8-dc9f-7d89d0181e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x32xH65gYiYp",
        "colab_type": "code",
        "outputId": "b68fcd59-9138-40fb-bd98-e4202c0aff2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My\\ Drive/465data/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/465data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iqKtlLYJCwD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "96597751-5b45-4c39-817d-bf09f2f6a0ed"
      },
      "source": [
        "ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32ptx_data_n                         HAM10000_images_part_2labels_np.npy\n",
            "HAM10000_images_part_2images_np.npy  images_np_32.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGCsFWh0ZXyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(\"32ptx_data_n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr5aviudaAh-",
        "colab_type": "code",
        "outputId": "a74071ef-60a6-497c-a55b-6c51a5de146b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "print(\"Shape of dataset\", dataset.shape)\n",
        "dataset.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of dataset (9993, 3080)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>3039</th>\n",
              "      <th>3040</th>\n",
              "      <th>3041</th>\n",
              "      <th>3042</th>\n",
              "      <th>3043</th>\n",
              "      <th>3044</th>\n",
              "      <th>3045</th>\n",
              "      <th>3046</th>\n",
              "      <th>3047</th>\n",
              "      <th>3048</th>\n",
              "      <th>3049</th>\n",
              "      <th>3050</th>\n",
              "      <th>3051</th>\n",
              "      <th>3052</th>\n",
              "      <th>3053</th>\n",
              "      <th>3054</th>\n",
              "      <th>3055</th>\n",
              "      <th>3056</th>\n",
              "      <th>3057</th>\n",
              "      <th>3058</th>\n",
              "      <th>3059</th>\n",
              "      <th>3060</th>\n",
              "      <th>3061</th>\n",
              "      <th>3062</th>\n",
              "      <th>3063</th>\n",
              "      <th>3064</th>\n",
              "      <th>3065</th>\n",
              "      <th>3066</th>\n",
              "      <th>3067</th>\n",
              "      <th>3068</th>\n",
              "      <th>3069</th>\n",
              "      <th>3070</th>\n",
              "      <th>3071</th>\n",
              "      <th>image_id</th>\n",
              "      <th>lesion_id</th>\n",
              "      <th>dx</th>\n",
              "      <th>dx_type</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>localization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>229</td>\n",
              "      <td>128</td>\n",
              "      <td>132</td>\n",
              "      <td>232</td>\n",
              "      <td>126</td>\n",
              "      <td>128</td>\n",
              "      <td>230</td>\n",
              "      <td>128</td>\n",
              "      <td>127</td>\n",
              "      <td>233</td>\n",
              "      <td>131</td>\n",
              "      <td>133</td>\n",
              "      <td>238</td>\n",
              "      <td>134</td>\n",
              "      <td>141</td>\n",
              "      <td>237</td>\n",
              "      <td>133</td>\n",
              "      <td>141</td>\n",
              "      <td>238</td>\n",
              "      <td>136</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>132</td>\n",
              "      <td>135</td>\n",
              "      <td>237</td>\n",
              "      <td>132</td>\n",
              "      <td>139</td>\n",
              "      <td>242</td>\n",
              "      <td>141</td>\n",
              "      <td>157</td>\n",
              "      <td>244</td>\n",
              "      <td>143</td>\n",
              "      <td>156</td>\n",
              "      <td>244</td>\n",
              "      <td>151</td>\n",
              "      <td>160</td>\n",
              "      <td>245</td>\n",
              "      <td>150</td>\n",
              "      <td>160</td>\n",
              "      <td>...</td>\n",
              "      <td>228</td>\n",
              "      <td>140</td>\n",
              "      <td>137</td>\n",
              "      <td>231</td>\n",
              "      <td>144</td>\n",
              "      <td>138</td>\n",
              "      <td>227</td>\n",
              "      <td>141</td>\n",
              "      <td>130</td>\n",
              "      <td>224</td>\n",
              "      <td>140</td>\n",
              "      <td>129</td>\n",
              "      <td>227</td>\n",
              "      <td>145</td>\n",
              "      <td>133</td>\n",
              "      <td>225</td>\n",
              "      <td>142</td>\n",
              "      <td>128</td>\n",
              "      <td>224</td>\n",
              "      <td>144</td>\n",
              "      <td>130</td>\n",
              "      <td>223</td>\n",
              "      <td>148</td>\n",
              "      <td>136</td>\n",
              "      <td>224</td>\n",
              "      <td>150</td>\n",
              "      <td>139</td>\n",
              "      <td>222</td>\n",
              "      <td>150</td>\n",
              "      <td>138</td>\n",
              "      <td>215</td>\n",
              "      <td>145</td>\n",
              "      <td>133</td>\n",
              "      <td>ISIC_0024306</td>\n",
              "      <td>HAM_0000550</td>\n",
              "      <td>nv</td>\n",
              "      <td>follow_up</td>\n",
              "      <td>45.0</td>\n",
              "      <td>male</td>\n",
              "      <td>trunk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>216</td>\n",
              "      <td>122</td>\n",
              "      <td>134</td>\n",
              "      <td>217</td>\n",
              "      <td>120</td>\n",
              "      <td>132</td>\n",
              "      <td>217</td>\n",
              "      <td>117</td>\n",
              "      <td>133</td>\n",
              "      <td>218</td>\n",
              "      <td>120</td>\n",
              "      <td>133</td>\n",
              "      <td>215</td>\n",
              "      <td>116</td>\n",
              "      <td>123</td>\n",
              "      <td>216</td>\n",
              "      <td>118</td>\n",
              "      <td>124</td>\n",
              "      <td>218</td>\n",
              "      <td>122</td>\n",
              "      <td>134</td>\n",
              "      <td>222</td>\n",
              "      <td>127</td>\n",
              "      <td>142</td>\n",
              "      <td>225</td>\n",
              "      <td>133</td>\n",
              "      <td>148</td>\n",
              "      <td>221</td>\n",
              "      <td>128</td>\n",
              "      <td>143</td>\n",
              "      <td>227</td>\n",
              "      <td>134</td>\n",
              "      <td>153</td>\n",
              "      <td>234</td>\n",
              "      <td>143</td>\n",
              "      <td>162</td>\n",
              "      <td>233</td>\n",
              "      <td>143</td>\n",
              "      <td>162</td>\n",
              "      <td>...</td>\n",
              "      <td>226</td>\n",
              "      <td>144</td>\n",
              "      <td>150</td>\n",
              "      <td>226</td>\n",
              "      <td>144</td>\n",
              "      <td>151</td>\n",
              "      <td>222</td>\n",
              "      <td>143</td>\n",
              "      <td>151</td>\n",
              "      <td>224</td>\n",
              "      <td>146</td>\n",
              "      <td>153</td>\n",
              "      <td>223</td>\n",
              "      <td>147</td>\n",
              "      <td>154</td>\n",
              "      <td>218</td>\n",
              "      <td>143</td>\n",
              "      <td>149</td>\n",
              "      <td>217</td>\n",
              "      <td>142</td>\n",
              "      <td>148</td>\n",
              "      <td>219</td>\n",
              "      <td>144</td>\n",
              "      <td>150</td>\n",
              "      <td>213</td>\n",
              "      <td>141</td>\n",
              "      <td>145</td>\n",
              "      <td>214</td>\n",
              "      <td>140</td>\n",
              "      <td>145</td>\n",
              "      <td>208</td>\n",
              "      <td>136</td>\n",
              "      <td>139</td>\n",
              "      <td>ISIC_0024307</td>\n",
              "      <td>HAM_0003577</td>\n",
              "      <td>nv</td>\n",
              "      <td>follow_up</td>\n",
              "      <td>50.0</td>\n",
              "      <td>male</td>\n",
              "      <td>lower extremity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>245</td>\n",
              "      <td>171</td>\n",
              "      <td>177</td>\n",
              "      <td>246</td>\n",
              "      <td>173</td>\n",
              "      <td>180</td>\n",
              "      <td>247</td>\n",
              "      <td>174</td>\n",
              "      <td>182</td>\n",
              "      <td>247</td>\n",
              "      <td>175</td>\n",
              "      <td>182</td>\n",
              "      <td>248</td>\n",
              "      <td>180</td>\n",
              "      <td>187</td>\n",
              "      <td>248</td>\n",
              "      <td>181</td>\n",
              "      <td>188</td>\n",
              "      <td>248</td>\n",
              "      <td>179</td>\n",
              "      <td>185</td>\n",
              "      <td>248</td>\n",
              "      <td>180</td>\n",
              "      <td>186</td>\n",
              "      <td>248</td>\n",
              "      <td>182</td>\n",
              "      <td>188</td>\n",
              "      <td>248</td>\n",
              "      <td>182</td>\n",
              "      <td>187</td>\n",
              "      <td>248</td>\n",
              "      <td>182</td>\n",
              "      <td>186</td>\n",
              "      <td>248</td>\n",
              "      <td>182</td>\n",
              "      <td>186</td>\n",
              "      <td>248</td>\n",
              "      <td>185</td>\n",
              "      <td>188</td>\n",
              "      <td>...</td>\n",
              "      <td>229</td>\n",
              "      <td>158</td>\n",
              "      <td>162</td>\n",
              "      <td>228</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>225</td>\n",
              "      <td>150</td>\n",
              "      <td>147</td>\n",
              "      <td>223</td>\n",
              "      <td>145</td>\n",
              "      <td>145</td>\n",
              "      <td>221</td>\n",
              "      <td>145</td>\n",
              "      <td>147</td>\n",
              "      <td>215</td>\n",
              "      <td>136</td>\n",
              "      <td>138</td>\n",
              "      <td>215</td>\n",
              "      <td>136</td>\n",
              "      <td>140</td>\n",
              "      <td>213</td>\n",
              "      <td>136</td>\n",
              "      <td>144</td>\n",
              "      <td>213</td>\n",
              "      <td>139</td>\n",
              "      <td>149</td>\n",
              "      <td>217</td>\n",
              "      <td>147</td>\n",
              "      <td>155</td>\n",
              "      <td>215</td>\n",
              "      <td>144</td>\n",
              "      <td>154</td>\n",
              "      <td>ISIC_0024308</td>\n",
              "      <td>HAM_0001477</td>\n",
              "      <td>nv</td>\n",
              "      <td>follow_up</td>\n",
              "      <td>55.0</td>\n",
              "      <td>female</td>\n",
              "      <td>trunk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>209</td>\n",
              "      <td>121</td>\n",
              "      <td>135</td>\n",
              "      <td>209</td>\n",
              "      <td>118</td>\n",
              "      <td>134</td>\n",
              "      <td>210</td>\n",
              "      <td>122</td>\n",
              "      <td>137</td>\n",
              "      <td>211</td>\n",
              "      <td>125</td>\n",
              "      <td>140</td>\n",
              "      <td>212</td>\n",
              "      <td>123</td>\n",
              "      <td>139</td>\n",
              "      <td>212</td>\n",
              "      <td>121</td>\n",
              "      <td>138</td>\n",
              "      <td>213</td>\n",
              "      <td>123</td>\n",
              "      <td>140</td>\n",
              "      <td>216</td>\n",
              "      <td>133</td>\n",
              "      <td>146</td>\n",
              "      <td>218</td>\n",
              "      <td>135</td>\n",
              "      <td>151</td>\n",
              "      <td>217</td>\n",
              "      <td>126</td>\n",
              "      <td>143</td>\n",
              "      <td>217</td>\n",
              "      <td>127</td>\n",
              "      <td>138</td>\n",
              "      <td>222</td>\n",
              "      <td>139</td>\n",
              "      <td>148</td>\n",
              "      <td>224</td>\n",
              "      <td>145</td>\n",
              "      <td>156</td>\n",
              "      <td>...</td>\n",
              "      <td>197</td>\n",
              "      <td>120</td>\n",
              "      <td>130</td>\n",
              "      <td>192</td>\n",
              "      <td>117</td>\n",
              "      <td>127</td>\n",
              "      <td>191</td>\n",
              "      <td>118</td>\n",
              "      <td>126</td>\n",
              "      <td>196</td>\n",
              "      <td>122</td>\n",
              "      <td>127</td>\n",
              "      <td>198</td>\n",
              "      <td>127</td>\n",
              "      <td>127</td>\n",
              "      <td>199</td>\n",
              "      <td>129</td>\n",
              "      <td>127</td>\n",
              "      <td>197</td>\n",
              "      <td>126</td>\n",
              "      <td>121</td>\n",
              "      <td>195</td>\n",
              "      <td>123</td>\n",
              "      <td>119</td>\n",
              "      <td>193</td>\n",
              "      <td>121</td>\n",
              "      <td>118</td>\n",
              "      <td>190</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>189</td>\n",
              "      <td>122</td>\n",
              "      <td>125</td>\n",
              "      <td>ISIC_0024309</td>\n",
              "      <td>HAM_0000484</td>\n",
              "      <td>nv</td>\n",
              "      <td>follow_up</td>\n",
              "      <td>40.0</td>\n",
              "      <td>male</td>\n",
              "      <td>trunk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>28</td>\n",
              "      <td>34</td>\n",
              "      <td>92</td>\n",
              "      <td>45</td>\n",
              "      <td>50</td>\n",
              "      <td>190</td>\n",
              "      <td>129</td>\n",
              "      <td>134</td>\n",
              "      <td>210</td>\n",
              "      <td>151</td>\n",
              "      <td>157</td>\n",
              "      <td>210</td>\n",
              "      <td>141</td>\n",
              "      <td>159</td>\n",
              "      <td>212</td>\n",
              "      <td>147</td>\n",
              "      <td>159</td>\n",
              "      <td>215</td>\n",
              "      <td>161</td>\n",
              "      <td>170</td>\n",
              "      <td>217</td>\n",
              "      <td>160</td>\n",
              "      <td>174</td>\n",
              "      <td>214</td>\n",
              "      <td>150</td>\n",
              "      <td>153</td>\n",
              "      <td>220</td>\n",
              "      <td>169</td>\n",
              "      <td>177</td>\n",
              "      <td>214</td>\n",
              "      <td>158</td>\n",
              "      <td>149</td>\n",
              "      <td>208</td>\n",
              "      <td>140</td>\n",
              "      <td>122</td>\n",
              "      <td>216</td>\n",
              "      <td>153</td>\n",
              "      <td>150</td>\n",
              "      <td>...</td>\n",
              "      <td>215</td>\n",
              "      <td>150</td>\n",
              "      <td>156</td>\n",
              "      <td>221</td>\n",
              "      <td>167</td>\n",
              "      <td>187</td>\n",
              "      <td>214</td>\n",
              "      <td>152</td>\n",
              "      <td>156</td>\n",
              "      <td>205</td>\n",
              "      <td>123</td>\n",
              "      <td>104</td>\n",
              "      <td>213</td>\n",
              "      <td>149</td>\n",
              "      <td>154</td>\n",
              "      <td>208</td>\n",
              "      <td>144</td>\n",
              "      <td>131</td>\n",
              "      <td>203</td>\n",
              "      <td>126</td>\n",
              "      <td>108</td>\n",
              "      <td>207</td>\n",
              "      <td>135</td>\n",
              "      <td>138</td>\n",
              "      <td>206</td>\n",
              "      <td>137</td>\n",
              "      <td>141</td>\n",
              "      <td>140</td>\n",
              "      <td>85</td>\n",
              "      <td>85</td>\n",
              "      <td>69</td>\n",
              "      <td>30</td>\n",
              "      <td>41</td>\n",
              "      <td>ISIC_0024310</td>\n",
              "      <td>HAM_0003350</td>\n",
              "      <td>mel</td>\n",
              "      <td>histo</td>\n",
              "      <td>60.0</td>\n",
              "      <td>male</td>\n",
              "      <td>chest</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3080 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    0    1    2  ...    dx_type   age     sex     localization\n",
              "0           0  229  128  132  ...  follow_up  45.0    male            trunk\n",
              "1           1  216  122  134  ...  follow_up  50.0    male  lower extremity\n",
              "2           2  245  171  177  ...  follow_up  55.0  female            trunk\n",
              "3           3  209  121  135  ...  follow_up  40.0    male            trunk\n",
              "4           4   64   28   34  ...      histo  60.0    male            chest\n",
              "\n",
              "[5 rows x 3080 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shHco0WgJU3O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "b04f05e6-a6ef-4f9d-e3a7-6c4316ef309d"
      },
      "source": [
        "X = dataset.drop(columns = [\"Unnamed: 0\",\"dx\",\"image_id\",\"lesion_id\",\"dx\",\"dx_type\",\"age\",\"sex\",\"localization\"])\n",
        "print(\"Shape of X\", X.shape)\n",
        "X.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X (9993, 3072)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>3032</th>\n",
              "      <th>3033</th>\n",
              "      <th>3034</th>\n",
              "      <th>3035</th>\n",
              "      <th>3036</th>\n",
              "      <th>3037</th>\n",
              "      <th>3038</th>\n",
              "      <th>3039</th>\n",
              "      <th>3040</th>\n",
              "      <th>3041</th>\n",
              "      <th>3042</th>\n",
              "      <th>3043</th>\n",
              "      <th>3044</th>\n",
              "      <th>3045</th>\n",
              "      <th>3046</th>\n",
              "      <th>3047</th>\n",
              "      <th>3048</th>\n",
              "      <th>3049</th>\n",
              "      <th>3050</th>\n",
              "      <th>3051</th>\n",
              "      <th>3052</th>\n",
              "      <th>3053</th>\n",
              "      <th>3054</th>\n",
              "      <th>3055</th>\n",
              "      <th>3056</th>\n",
              "      <th>3057</th>\n",
              "      <th>3058</th>\n",
              "      <th>3059</th>\n",
              "      <th>3060</th>\n",
              "      <th>3061</th>\n",
              "      <th>3062</th>\n",
              "      <th>3063</th>\n",
              "      <th>3064</th>\n",
              "      <th>3065</th>\n",
              "      <th>3066</th>\n",
              "      <th>3067</th>\n",
              "      <th>3068</th>\n",
              "      <th>3069</th>\n",
              "      <th>3070</th>\n",
              "      <th>3071</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>229</td>\n",
              "      <td>128</td>\n",
              "      <td>132</td>\n",
              "      <td>232</td>\n",
              "      <td>126</td>\n",
              "      <td>128</td>\n",
              "      <td>230</td>\n",
              "      <td>128</td>\n",
              "      <td>127</td>\n",
              "      <td>233</td>\n",
              "      <td>131</td>\n",
              "      <td>133</td>\n",
              "      <td>238</td>\n",
              "      <td>134</td>\n",
              "      <td>141</td>\n",
              "      <td>237</td>\n",
              "      <td>133</td>\n",
              "      <td>141</td>\n",
              "      <td>238</td>\n",
              "      <td>136</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>132</td>\n",
              "      <td>135</td>\n",
              "      <td>237</td>\n",
              "      <td>132</td>\n",
              "      <td>139</td>\n",
              "      <td>242</td>\n",
              "      <td>141</td>\n",
              "      <td>157</td>\n",
              "      <td>244</td>\n",
              "      <td>143</td>\n",
              "      <td>156</td>\n",
              "      <td>244</td>\n",
              "      <td>151</td>\n",
              "      <td>160</td>\n",
              "      <td>245</td>\n",
              "      <td>150</td>\n",
              "      <td>160</td>\n",
              "      <td>243</td>\n",
              "      <td>...</td>\n",
              "      <td>147</td>\n",
              "      <td>229</td>\n",
              "      <td>144</td>\n",
              "      <td>142</td>\n",
              "      <td>226</td>\n",
              "      <td>136</td>\n",
              "      <td>132</td>\n",
              "      <td>228</td>\n",
              "      <td>140</td>\n",
              "      <td>137</td>\n",
              "      <td>231</td>\n",
              "      <td>144</td>\n",
              "      <td>138</td>\n",
              "      <td>227</td>\n",
              "      <td>141</td>\n",
              "      <td>130</td>\n",
              "      <td>224</td>\n",
              "      <td>140</td>\n",
              "      <td>129</td>\n",
              "      <td>227</td>\n",
              "      <td>145</td>\n",
              "      <td>133</td>\n",
              "      <td>225</td>\n",
              "      <td>142</td>\n",
              "      <td>128</td>\n",
              "      <td>224</td>\n",
              "      <td>144</td>\n",
              "      <td>130</td>\n",
              "      <td>223</td>\n",
              "      <td>148</td>\n",
              "      <td>136</td>\n",
              "      <td>224</td>\n",
              "      <td>150</td>\n",
              "      <td>139</td>\n",
              "      <td>222</td>\n",
              "      <td>150</td>\n",
              "      <td>138</td>\n",
              "      <td>215</td>\n",
              "      <td>145</td>\n",
              "      <td>133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>216</td>\n",
              "      <td>122</td>\n",
              "      <td>134</td>\n",
              "      <td>217</td>\n",
              "      <td>120</td>\n",
              "      <td>132</td>\n",
              "      <td>217</td>\n",
              "      <td>117</td>\n",
              "      <td>133</td>\n",
              "      <td>218</td>\n",
              "      <td>120</td>\n",
              "      <td>133</td>\n",
              "      <td>215</td>\n",
              "      <td>116</td>\n",
              "      <td>123</td>\n",
              "      <td>216</td>\n",
              "      <td>118</td>\n",
              "      <td>124</td>\n",
              "      <td>218</td>\n",
              "      <td>122</td>\n",
              "      <td>134</td>\n",
              "      <td>222</td>\n",
              "      <td>127</td>\n",
              "      <td>142</td>\n",
              "      <td>225</td>\n",
              "      <td>133</td>\n",
              "      <td>148</td>\n",
              "      <td>221</td>\n",
              "      <td>128</td>\n",
              "      <td>143</td>\n",
              "      <td>227</td>\n",
              "      <td>134</td>\n",
              "      <td>153</td>\n",
              "      <td>234</td>\n",
              "      <td>143</td>\n",
              "      <td>162</td>\n",
              "      <td>233</td>\n",
              "      <td>143</td>\n",
              "      <td>162</td>\n",
              "      <td>240</td>\n",
              "      <td>...</td>\n",
              "      <td>144</td>\n",
              "      <td>220</td>\n",
              "      <td>138</td>\n",
              "      <td>144</td>\n",
              "      <td>217</td>\n",
              "      <td>136</td>\n",
              "      <td>142</td>\n",
              "      <td>226</td>\n",
              "      <td>144</td>\n",
              "      <td>150</td>\n",
              "      <td>226</td>\n",
              "      <td>144</td>\n",
              "      <td>151</td>\n",
              "      <td>222</td>\n",
              "      <td>143</td>\n",
              "      <td>151</td>\n",
              "      <td>224</td>\n",
              "      <td>146</td>\n",
              "      <td>153</td>\n",
              "      <td>223</td>\n",
              "      <td>147</td>\n",
              "      <td>154</td>\n",
              "      <td>218</td>\n",
              "      <td>143</td>\n",
              "      <td>149</td>\n",
              "      <td>217</td>\n",
              "      <td>142</td>\n",
              "      <td>148</td>\n",
              "      <td>219</td>\n",
              "      <td>144</td>\n",
              "      <td>150</td>\n",
              "      <td>213</td>\n",
              "      <td>141</td>\n",
              "      <td>145</td>\n",
              "      <td>214</td>\n",
              "      <td>140</td>\n",
              "      <td>145</td>\n",
              "      <td>208</td>\n",
              "      <td>136</td>\n",
              "      <td>139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>245</td>\n",
              "      <td>171</td>\n",
              "      <td>177</td>\n",
              "      <td>246</td>\n",
              "      <td>173</td>\n",
              "      <td>180</td>\n",
              "      <td>247</td>\n",
              "      <td>174</td>\n",
              "      <td>182</td>\n",
              "      <td>247</td>\n",
              "      <td>175</td>\n",
              "      <td>182</td>\n",
              "      <td>248</td>\n",
              "      <td>180</td>\n",
              "      <td>187</td>\n",
              "      <td>248</td>\n",
              "      <td>181</td>\n",
              "      <td>188</td>\n",
              "      <td>248</td>\n",
              "      <td>179</td>\n",
              "      <td>185</td>\n",
              "      <td>248</td>\n",
              "      <td>180</td>\n",
              "      <td>186</td>\n",
              "      <td>248</td>\n",
              "      <td>182</td>\n",
              "      <td>188</td>\n",
              "      <td>248</td>\n",
              "      <td>182</td>\n",
              "      <td>187</td>\n",
              "      <td>248</td>\n",
              "      <td>182</td>\n",
              "      <td>186</td>\n",
              "      <td>248</td>\n",
              "      <td>182</td>\n",
              "      <td>186</td>\n",
              "      <td>248</td>\n",
              "      <td>185</td>\n",
              "      <td>188</td>\n",
              "      <td>248</td>\n",
              "      <td>...</td>\n",
              "      <td>171</td>\n",
              "      <td>236</td>\n",
              "      <td>164</td>\n",
              "      <td>168</td>\n",
              "      <td>233</td>\n",
              "      <td>161</td>\n",
              "      <td>168</td>\n",
              "      <td>229</td>\n",
              "      <td>158</td>\n",
              "      <td>162</td>\n",
              "      <td>228</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>225</td>\n",
              "      <td>150</td>\n",
              "      <td>147</td>\n",
              "      <td>223</td>\n",
              "      <td>145</td>\n",
              "      <td>145</td>\n",
              "      <td>221</td>\n",
              "      <td>145</td>\n",
              "      <td>147</td>\n",
              "      <td>215</td>\n",
              "      <td>136</td>\n",
              "      <td>138</td>\n",
              "      <td>215</td>\n",
              "      <td>136</td>\n",
              "      <td>140</td>\n",
              "      <td>213</td>\n",
              "      <td>136</td>\n",
              "      <td>144</td>\n",
              "      <td>213</td>\n",
              "      <td>139</td>\n",
              "      <td>149</td>\n",
              "      <td>217</td>\n",
              "      <td>147</td>\n",
              "      <td>155</td>\n",
              "      <td>215</td>\n",
              "      <td>144</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>209</td>\n",
              "      <td>121</td>\n",
              "      <td>135</td>\n",
              "      <td>209</td>\n",
              "      <td>118</td>\n",
              "      <td>134</td>\n",
              "      <td>210</td>\n",
              "      <td>122</td>\n",
              "      <td>137</td>\n",
              "      <td>211</td>\n",
              "      <td>125</td>\n",
              "      <td>140</td>\n",
              "      <td>212</td>\n",
              "      <td>123</td>\n",
              "      <td>139</td>\n",
              "      <td>212</td>\n",
              "      <td>121</td>\n",
              "      <td>138</td>\n",
              "      <td>213</td>\n",
              "      <td>123</td>\n",
              "      <td>140</td>\n",
              "      <td>216</td>\n",
              "      <td>133</td>\n",
              "      <td>146</td>\n",
              "      <td>218</td>\n",
              "      <td>135</td>\n",
              "      <td>151</td>\n",
              "      <td>217</td>\n",
              "      <td>126</td>\n",
              "      <td>143</td>\n",
              "      <td>217</td>\n",
              "      <td>127</td>\n",
              "      <td>138</td>\n",
              "      <td>222</td>\n",
              "      <td>139</td>\n",
              "      <td>148</td>\n",
              "      <td>224</td>\n",
              "      <td>145</td>\n",
              "      <td>156</td>\n",
              "      <td>228</td>\n",
              "      <td>...</td>\n",
              "      <td>116</td>\n",
              "      <td>194</td>\n",
              "      <td>107</td>\n",
              "      <td>116</td>\n",
              "      <td>197</td>\n",
              "      <td>115</td>\n",
              "      <td>127</td>\n",
              "      <td>197</td>\n",
              "      <td>120</td>\n",
              "      <td>130</td>\n",
              "      <td>192</td>\n",
              "      <td>117</td>\n",
              "      <td>127</td>\n",
              "      <td>191</td>\n",
              "      <td>118</td>\n",
              "      <td>126</td>\n",
              "      <td>196</td>\n",
              "      <td>122</td>\n",
              "      <td>127</td>\n",
              "      <td>198</td>\n",
              "      <td>127</td>\n",
              "      <td>127</td>\n",
              "      <td>199</td>\n",
              "      <td>129</td>\n",
              "      <td>127</td>\n",
              "      <td>197</td>\n",
              "      <td>126</td>\n",
              "      <td>121</td>\n",
              "      <td>195</td>\n",
              "      <td>123</td>\n",
              "      <td>119</td>\n",
              "      <td>193</td>\n",
              "      <td>121</td>\n",
              "      <td>118</td>\n",
              "      <td>190</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>189</td>\n",
              "      <td>122</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>28</td>\n",
              "      <td>34</td>\n",
              "      <td>92</td>\n",
              "      <td>45</td>\n",
              "      <td>50</td>\n",
              "      <td>190</td>\n",
              "      <td>129</td>\n",
              "      <td>134</td>\n",
              "      <td>210</td>\n",
              "      <td>151</td>\n",
              "      <td>157</td>\n",
              "      <td>210</td>\n",
              "      <td>141</td>\n",
              "      <td>159</td>\n",
              "      <td>212</td>\n",
              "      <td>147</td>\n",
              "      <td>159</td>\n",
              "      <td>215</td>\n",
              "      <td>161</td>\n",
              "      <td>170</td>\n",
              "      <td>217</td>\n",
              "      <td>160</td>\n",
              "      <td>174</td>\n",
              "      <td>214</td>\n",
              "      <td>150</td>\n",
              "      <td>153</td>\n",
              "      <td>220</td>\n",
              "      <td>169</td>\n",
              "      <td>177</td>\n",
              "      <td>214</td>\n",
              "      <td>158</td>\n",
              "      <td>149</td>\n",
              "      <td>208</td>\n",
              "      <td>140</td>\n",
              "      <td>122</td>\n",
              "      <td>216</td>\n",
              "      <td>153</td>\n",
              "      <td>150</td>\n",
              "      <td>219</td>\n",
              "      <td>...</td>\n",
              "      <td>144</td>\n",
              "      <td>215</td>\n",
              "      <td>146</td>\n",
              "      <td>142</td>\n",
              "      <td>216</td>\n",
              "      <td>150</td>\n",
              "      <td>153</td>\n",
              "      <td>215</td>\n",
              "      <td>150</td>\n",
              "      <td>156</td>\n",
              "      <td>221</td>\n",
              "      <td>167</td>\n",
              "      <td>187</td>\n",
              "      <td>214</td>\n",
              "      <td>152</td>\n",
              "      <td>156</td>\n",
              "      <td>205</td>\n",
              "      <td>123</td>\n",
              "      <td>104</td>\n",
              "      <td>213</td>\n",
              "      <td>149</td>\n",
              "      <td>154</td>\n",
              "      <td>208</td>\n",
              "      <td>144</td>\n",
              "      <td>131</td>\n",
              "      <td>203</td>\n",
              "      <td>126</td>\n",
              "      <td>108</td>\n",
              "      <td>207</td>\n",
              "      <td>135</td>\n",
              "      <td>138</td>\n",
              "      <td>206</td>\n",
              "      <td>137</td>\n",
              "      <td>141</td>\n",
              "      <td>140</td>\n",
              "      <td>85</td>\n",
              "      <td>85</td>\n",
              "      <td>69</td>\n",
              "      <td>30</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3072 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4    5    6  ...  3065  3066  3067  3068  3069  3070  3071\n",
              "0  229  128  132  232  126  128  230  ...   139   222   150   138   215   145   133\n",
              "1  216  122  134  217  120  132  217  ...   145   214   140   145   208   136   139\n",
              "2  245  171  177  246  173  180  247  ...   149   217   147   155   215   144   154\n",
              "3  209  121  135  209  118  134  210  ...   118   190   120   120   189   122   125\n",
              "4   64   28   34   92   45   50  190  ...   141   140    85    85    69    30    41\n",
              "\n",
              "[5 rows x 3072 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INXUUB4qLg7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "8ceb2edf-870f-4c30-beb4-da8900d484c7"
      },
      "source": [
        "Y = dataset[\"dx\"]\n",
        "Y = pd.get_dummies(Y)\n",
        "print(\"Shape of Y\", Y.shape)\n",
        "Y.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Y (9993, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>akiec</th>\n",
              "      <th>bcc</th>\n",
              "      <th>bkl</th>\n",
              "      <th>df</th>\n",
              "      <th>mel</th>\n",
              "      <th>nv</th>\n",
              "      <th>vasc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   akiec  bcc  bkl  df  mel  nv  vasc\n",
              "0      0    0    0   0    0   1     0\n",
              "1      0    0    0   0    0   1     0\n",
              "2      0    0    0   0    0   1     0\n",
              "3      0    0    0   0    0   1     0\n",
              "4      0    0    0   0    1   0     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiwaYj9Mcj5G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42511329-c8d2-4935-f436-38d873b8f174"
      },
      "source": [
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE1FzwJE8frE",
        "colab_type": "code",
        "outputId": "e443642f-913a-49aa-8b13-d6367975c5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 42)\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train_flat.shape[0]))\n",
        "print (\"number of test examples = \" + str(X_test_flat.shape[0]))\n",
        "print (\"X_train_flat shape: \" + str(X_train_flat.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_test_flat shape: \" + str(X_test_flat.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 8494\n",
            "number of test examples = 1499\n",
            "X_train_flat shape: (8494, 3072)\n",
            "Y_train shape: (8494, 7)\n",
            "X_test_flat shape: (1499, 3072)\n",
            "Y_test shape: (1499, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z04BiOiwM1nn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "e735f75e-83dd-49f5-97db-0644de9afcef"
      },
      "source": [
        "X_train = X_train.values\n",
        "Y_train = Y_train.values"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-d9dd612b457a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk6ZysigiUkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LRModel1(input_shape = (3072,), classes = 7):\n",
        "    X_input = Input(input_shape)\n",
        "    X = BatchNormalization()(X_input)\n",
        "    X = Dense(input_shape, activation='sigmoid', name = \"sigmoid_activation\", kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Dense(classes, input_shape=input_shape, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    model = Model(inputs = X_input, outputs = X, name='LogisticRegression')\n",
        "    return model\n",
        "\n",
        "def LRModel2(input_shape = (3072,), classes = 7):\n",
        "    X_input = Input(input_shape)\n",
        "    X = BatchNormalization()(X_input)\n",
        "    X = Dense(classes, input_shape=input_shape, activation='sigmoid', name = \"sigmoid_activation\", kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation(\"softmax\")(X)\n",
        "    model = Model(inputs = X_input, outputs = X, name='LogisticRegression')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhjujZxQkBQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LRModel2(input_shape = (3072,), classes = 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnE-VAFEkchs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD, Adam\n",
        "adam = Adam(lr= 0.0005)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSe6Fec-kg9_",
        "colab_type": "code",
        "outputId": "5f0990c4-fe37-4885-c4bd-d3e590db218c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, Y_train, epochs = 200, batch_size = 32)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "8494/8494 [==============================] - 1s 131us/step - loss: 1.4186 - accuracy: 0.6373\n",
            "Epoch 2/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4165 - accuracy: 0.6447\n",
            "Epoch 3/200\n",
            "8494/8494 [==============================] - 1s 131us/step - loss: 1.4140 - accuracy: 0.6447\n",
            "Epoch 4/200\n",
            "8494/8494 [==============================] - 1s 130us/step - loss: 1.4130 - accuracy: 0.6396\n",
            "Epoch 5/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4119 - accuracy: 0.6468\n",
            "Epoch 6/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4128 - accuracy: 0.6386\n",
            "Epoch 7/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4138 - accuracy: 0.6389\n",
            "Epoch 8/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4124 - accuracy: 0.6462\n",
            "Epoch 9/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4127 - accuracy: 0.6465\n",
            "Epoch 10/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4124 - accuracy: 0.6533\n",
            "Epoch 11/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4116 - accuracy: 0.6559\n",
            "Epoch 12/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4137 - accuracy: 0.6568\n",
            "Epoch 13/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4137 - accuracy: 0.6525\n",
            "Epoch 14/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4123 - accuracy: 0.6519\n",
            "Epoch 15/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4097 - accuracy: 0.6507\n",
            "Epoch 16/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4127 - accuracy: 0.6370\n",
            "Epoch 17/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4133 - accuracy: 0.6414\n",
            "Epoch 18/200\n",
            "8494/8494 [==============================] - 1s 130us/step - loss: 1.4145 - accuracy: 0.6426\n",
            "Epoch 19/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4110 - accuracy: 0.6420\n",
            "Epoch 20/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4135 - accuracy: 0.6387\n",
            "Epoch 21/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4132 - accuracy: 0.6421\n",
            "Epoch 22/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4083 - accuracy: 0.6489\n",
            "Epoch 23/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4119 - accuracy: 0.6455\n",
            "Epoch 24/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4108 - accuracy: 0.6502\n",
            "Epoch 25/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4123 - accuracy: 0.6479\n",
            "Epoch 26/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4118 - accuracy: 0.6516\n",
            "Epoch 27/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4111 - accuracy: 0.6572\n",
            "Epoch 28/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4135 - accuracy: 0.6468\n",
            "Epoch 29/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4104 - accuracy: 0.6522\n",
            "Epoch 30/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4122 - accuracy: 0.6467\n",
            "Epoch 31/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4117 - accuracy: 0.6479\n",
            "Epoch 32/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4139 - accuracy: 0.6476\n",
            "Epoch 33/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4102 - accuracy: 0.6554\n",
            "Epoch 34/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4100 - accuracy: 0.6585\n",
            "Epoch 35/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4116 - accuracy: 0.6592\n",
            "Epoch 36/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4137 - accuracy: 0.6486\n",
            "Epoch 37/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4125 - accuracy: 0.6573\n",
            "Epoch 38/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4120 - accuracy: 0.6576\n",
            "Epoch 39/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4118 - accuracy: 0.6540\n",
            "Epoch 40/200\n",
            "8494/8494 [==============================] - 1s 129us/step - loss: 1.4113 - accuracy: 0.6556\n",
            "Epoch 41/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4111 - accuracy: 0.6583\n",
            "Epoch 42/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4090 - accuracy: 0.6520\n",
            "Epoch 43/200\n",
            "8494/8494 [==============================] - 1s 121us/step - loss: 1.4133 - accuracy: 0.6483\n",
            "Epoch 44/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4081 - accuracy: 0.6496\n",
            "Epoch 45/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4088 - accuracy: 0.6527\n",
            "Epoch 46/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4124 - accuracy: 0.6505\n",
            "Epoch 47/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4122 - accuracy: 0.6459\n",
            "Epoch 48/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4085 - accuracy: 0.6473\n",
            "Epoch 49/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4106 - accuracy: 0.6455\n",
            "Epoch 50/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4100 - accuracy: 0.6469\n",
            "Epoch 51/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4101 - accuracy: 0.6474\n",
            "Epoch 52/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4102 - accuracy: 0.6468\n",
            "Epoch 53/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4088 - accuracy: 0.6489\n",
            "Epoch 54/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4104 - accuracy: 0.6589\n",
            "Epoch 55/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4134 - accuracy: 0.6553\n",
            "Epoch 56/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4087 - accuracy: 0.6566\n",
            "Epoch 57/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4106 - accuracy: 0.6394\n",
            "Epoch 58/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4105 - accuracy: 0.6475\n",
            "Epoch 59/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4105 - accuracy: 0.6433\n",
            "Epoch 60/200\n",
            "8494/8494 [==============================] - 1s 121us/step - loss: 1.4097 - accuracy: 0.6486\n",
            "Epoch 61/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4089 - accuracy: 0.6498\n",
            "Epoch 62/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4120 - accuracy: 0.6430\n",
            "Epoch 63/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4112 - accuracy: 0.6565\n",
            "Epoch 64/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4112 - accuracy: 0.6554\n",
            "Epoch 65/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4086 - accuracy: 0.6588\n",
            "Epoch 66/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4090 - accuracy: 0.6629\n",
            "Epoch 67/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4101 - accuracy: 0.6647\n",
            "Epoch 68/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4074 - accuracy: 0.6721\n",
            "Epoch 69/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4098 - accuracy: 0.6619\n",
            "Epoch 70/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4097 - accuracy: 0.6596\n",
            "Epoch 71/200\n",
            "8494/8494 [==============================] - 1s 130us/step - loss: 1.4093 - accuracy: 0.6619\n",
            "Epoch 72/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4115 - accuracy: 0.6599\n",
            "Epoch 73/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4108 - accuracy: 0.6620\n",
            "Epoch 74/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4104 - accuracy: 0.6612\n",
            "Epoch 75/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4088 - accuracy: 0.6615\n",
            "Epoch 76/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4095 - accuracy: 0.6600\n",
            "Epoch 77/200\n",
            "8494/8494 [==============================] - 1s 129us/step - loss: 1.4074 - accuracy: 0.6567\n",
            "Epoch 78/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4076 - accuracy: 0.6525\n",
            "Epoch 79/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4102 - accuracy: 0.6503\n",
            "Epoch 80/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4108 - accuracy: 0.6459\n",
            "Epoch 81/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4104 - accuracy: 0.6446\n",
            "Epoch 82/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4113 - accuracy: 0.6476\n",
            "Epoch 83/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4062 - accuracy: 0.6533\n",
            "Epoch 84/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4085 - accuracy: 0.6512\n",
            "Epoch 85/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4075 - accuracy: 0.6611\n",
            "Epoch 86/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4071 - accuracy: 0.6545\n",
            "Epoch 87/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4087 - accuracy: 0.6533\n",
            "Epoch 88/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4093 - accuracy: 0.6558\n",
            "Epoch 89/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4096 - accuracy: 0.6502\n",
            "Epoch 90/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4098 - accuracy: 0.6558\n",
            "Epoch 91/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4097 - accuracy: 0.6547\n",
            "Epoch 92/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4068 - accuracy: 0.6620\n",
            "Epoch 93/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4083 - accuracy: 0.6600\n",
            "Epoch 94/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4102 - accuracy: 0.6455\n",
            "Epoch 95/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4070 - accuracy: 0.6489\n",
            "Epoch 96/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4103 - accuracy: 0.6474\n",
            "Epoch 97/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4064 - accuracy: 0.6454\n",
            "Epoch 98/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4078 - accuracy: 0.6440\n",
            "Epoch 99/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4093 - accuracy: 0.6482\n",
            "Epoch 100/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4102 - accuracy: 0.6496\n",
            "Epoch 101/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4088 - accuracy: 0.6518\n",
            "Epoch 102/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4083 - accuracy: 0.6502\n",
            "Epoch 103/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4090 - accuracy: 0.6529\n",
            "Epoch 104/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4067 - accuracy: 0.6532\n",
            "Epoch 105/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4054 - accuracy: 0.6601\n",
            "Epoch 106/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4062 - accuracy: 0.6586\n",
            "Epoch 107/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4074 - accuracy: 0.6514\n",
            "Epoch 108/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4065 - accuracy: 0.6514\n",
            "Epoch 109/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4061 - accuracy: 0.6553\n",
            "Epoch 110/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4049 - accuracy: 0.6619\n",
            "Epoch 111/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4090 - accuracy: 0.6547\n",
            "Epoch 112/200\n",
            "8494/8494 [==============================] - 1s 130us/step - loss: 1.4069 - accuracy: 0.6559\n",
            "Epoch 113/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4089 - accuracy: 0.6567\n",
            "Epoch 114/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4105 - accuracy: 0.6546\n",
            "Epoch 115/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4092 - accuracy: 0.6605\n",
            "Epoch 116/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4055 - accuracy: 0.6652\n",
            "Epoch 117/200\n",
            "8494/8494 [==============================] - 1s 120us/step - loss: 1.4072 - accuracy: 0.6542\n",
            "Epoch 118/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4052 - accuracy: 0.6569\n",
            "Epoch 119/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4064 - accuracy: 0.6580\n",
            "Epoch 120/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4075 - accuracy: 0.6614\n",
            "Epoch 121/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4083 - accuracy: 0.6635\n",
            "Epoch 122/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4073 - accuracy: 0.6631\n",
            "Epoch 123/200\n",
            "8494/8494 [==============================] - 1s 121us/step - loss: 1.4063 - accuracy: 0.6595\n",
            "Epoch 124/200\n",
            "8494/8494 [==============================] - 1s 121us/step - loss: 1.4069 - accuracy: 0.6574\n",
            "Epoch 125/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4062 - accuracy: 0.6507\n",
            "Epoch 126/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4063 - accuracy: 0.6556\n",
            "Epoch 127/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4062 - accuracy: 0.6647\n",
            "Epoch 128/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4058 - accuracy: 0.6668\n",
            "Epoch 129/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4090 - accuracy: 0.6614\n",
            "Epoch 130/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4067 - accuracy: 0.6619\n",
            "Epoch 131/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4038 - accuracy: 0.6616\n",
            "Epoch 132/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4067 - accuracy: 0.6594\n",
            "Epoch 133/200\n",
            "8494/8494 [==============================] - 1s 129us/step - loss: 1.4038 - accuracy: 0.6664\n",
            "Epoch 134/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4059 - accuracy: 0.6661\n",
            "Epoch 135/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4061 - accuracy: 0.6656\n",
            "Epoch 136/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4057 - accuracy: 0.6680\n",
            "Epoch 137/200\n",
            "8494/8494 [==============================] - 1s 131us/step - loss: 1.4079 - accuracy: 0.6638\n",
            "Epoch 138/200\n",
            "8494/8494 [==============================] - 1s 130us/step - loss: 1.4062 - accuracy: 0.6629\n",
            "Epoch 139/200\n",
            "8494/8494 [==============================] - 1s 130us/step - loss: 1.4054 - accuracy: 0.6718\n",
            "Epoch 140/200\n",
            "8494/8494 [==============================] - 1s 121us/step - loss: 1.4078 - accuracy: 0.6688\n",
            "Epoch 141/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4058 - accuracy: 0.6667\n",
            "Epoch 142/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4053 - accuracy: 0.6672\n",
            "Epoch 143/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4043 - accuracy: 0.6711\n",
            "Epoch 144/200\n",
            "8494/8494 [==============================] - 1s 121us/step - loss: 1.4046 - accuracy: 0.6678\n",
            "Epoch 145/200\n",
            "8494/8494 [==============================] - 1s 120us/step - loss: 1.4060 - accuracy: 0.6647\n",
            "Epoch 146/200\n",
            "8494/8494 [==============================] - 1s 121us/step - loss: 1.4033 - accuracy: 0.6632\n",
            "Epoch 147/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4050 - accuracy: 0.6553\n",
            "Epoch 148/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4068 - accuracy: 0.6559\n",
            "Epoch 149/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4015 - accuracy: 0.6556\n",
            "Epoch 150/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4058 - accuracy: 0.6483\n",
            "Epoch 151/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4066 - accuracy: 0.6485\n",
            "Epoch 152/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4059 - accuracy: 0.6553\n",
            "Epoch 153/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4042 - accuracy: 0.6599\n",
            "Epoch 154/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4033 - accuracy: 0.6623\n",
            "Epoch 155/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4072 - accuracy: 0.6654\n",
            "Epoch 156/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4086 - accuracy: 0.6701\n",
            "Epoch 157/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4064 - accuracy: 0.6674\n",
            "Epoch 158/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4053 - accuracy: 0.6669\n",
            "Epoch 159/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4049 - accuracy: 0.6668\n",
            "Epoch 160/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4046 - accuracy: 0.6712\n",
            "Epoch 161/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4038 - accuracy: 0.6688\n",
            "Epoch 162/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4038 - accuracy: 0.6674\n",
            "Epoch 163/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4052 - accuracy: 0.6655\n",
            "Epoch 164/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4047 - accuracy: 0.6653\n",
            "Epoch 165/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4033 - accuracy: 0.6651\n",
            "Epoch 166/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4045 - accuracy: 0.6547\n",
            "Epoch 167/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4043 - accuracy: 0.6567\n",
            "Epoch 168/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4042 - accuracy: 0.6634\n",
            "Epoch 169/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4042 - accuracy: 0.6559\n",
            "Epoch 170/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4062 - accuracy: 0.6526\n",
            "Epoch 171/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4044 - accuracy: 0.6566\n",
            "Epoch 172/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4035 - accuracy: 0.6613\n",
            "Epoch 173/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4013 - accuracy: 0.6628\n",
            "Epoch 174/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4041 - accuracy: 0.6572\n",
            "Epoch 175/200\n",
            "8494/8494 [==============================] - 1s 127us/step - loss: 1.4053 - accuracy: 0.6487\n",
            "Epoch 176/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4049 - accuracy: 0.6480\n",
            "Epoch 177/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4045 - accuracy: 0.6559\n",
            "Epoch 178/200\n",
            "8494/8494 [==============================] - 1s 125us/step - loss: 1.4048 - accuracy: 0.6515\n",
            "Epoch 179/200\n",
            "8494/8494 [==============================] - 1s 121us/step - loss: 1.4050 - accuracy: 0.6581\n",
            "Epoch 180/200\n",
            "8494/8494 [==============================] - 1s 126us/step - loss: 1.4040 - accuracy: 0.6644\n",
            "Epoch 181/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4044 - accuracy: 0.6615\n",
            "Epoch 182/200\n",
            "8494/8494 [==============================] - 1s 129us/step - loss: 1.4019 - accuracy: 0.6609\n",
            "Epoch 183/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4019 - accuracy: 0.6629\n",
            "Epoch 184/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4027 - accuracy: 0.6635\n",
            "Epoch 185/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4033 - accuracy: 0.6654\n",
            "Epoch 186/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4019 - accuracy: 0.6686\n",
            "Epoch 187/200\n",
            "8494/8494 [==============================] - 1s 131us/step - loss: 1.4028 - accuracy: 0.6672\n",
            "Epoch 188/200\n",
            "8494/8494 [==============================] - 1s 130us/step - loss: 1.4054 - accuracy: 0.6689\n",
            "Epoch 189/200\n",
            "8494/8494 [==============================] - 1s 129us/step - loss: 1.4050 - accuracy: 0.6717\n",
            "Epoch 190/200\n",
            "8494/8494 [==============================] - 1s 131us/step - loss: 1.4025 - accuracy: 0.6639\n",
            "Epoch 191/200\n",
            "8494/8494 [==============================] - 1s 132us/step - loss: 1.4016 - accuracy: 0.6629\n",
            "Epoch 192/200\n",
            "8494/8494 [==============================] - 1s 136us/step - loss: 1.4029 - accuracy: 0.6623\n",
            "Epoch 193/200\n",
            "8494/8494 [==============================] - 1s 138us/step - loss: 1.4005 - accuracy: 0.6587\n",
            "Epoch 194/200\n",
            "8494/8494 [==============================] - 1s 137us/step - loss: 1.4017 - accuracy: 0.6602\n",
            "Epoch 195/200\n",
            "8494/8494 [==============================] - 1s 128us/step - loss: 1.4007 - accuracy: 0.6689\n",
            "Epoch 196/200\n",
            "8494/8494 [==============================] - 1s 123us/step - loss: 1.4026 - accuracy: 0.6678\n",
            "Epoch 197/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4016 - accuracy: 0.6668\n",
            "Epoch 198/200\n",
            "8494/8494 [==============================] - 1s 124us/step - loss: 1.4031 - accuracy: 0.6633\n",
            "Epoch 199/200\n",
            "8494/8494 [==============================] - 1s 122us/step - loss: 1.4014 - accuracy: 0.6620\n",
            "Epoch 200/200\n",
            "8494/8494 [==============================] - 1s 129us/step - loss: 1.4036 - accuracy: 0.6668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbb49be17b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeDGWSws_c2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = X_test.values\n",
        "Y_test = Y_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBJFHwFH-hrX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6795c9a8-dab9-417a-d09f-cae3de557ddf"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, average_precision_score, f1_score\n",
        "y_pred_test = model.predict(X_test)\n",
        "y_pred_train = model.predict(X_train)\n",
        "conf_m_test = confusion_matrix(Y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
        "y_test_precision = average_precision_score(Y_test, y_pred_test)\n",
        "y_test_acc1 = accuracy_score(Y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
        "y_test_f1 = f1_score(Y_test.argmax(axis=1), y_pred_test.argmax(axis=1), average = 'weighted')\n",
        "y_train_f1 = f1_score(Y_train.argmax(axis=1), y_pred_train.argmax(axis=1), average = 'weighted')\n",
        "print(\"Precision of the Logistic Regression model in test set: \" + str(y_test_precision) + \"\\n Accuracy of the Logistic Regression Modelin test set: \" + str(y_test_acc1))\n",
        "print(\"f1 of the Logistic Regression model in test set: \" + str(y_test_f1))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision of the Logistic Regression model in test set: 0.25005542865124253\n",
            " Accuracy of the Logistic Regression Modelin test set: 0.6490993995997332\n",
            "f1 of the Logistic Regression model in test set: 0.6512995453824352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UfY3FzoSh_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3c6f9668-c55e-4701-9462-e443b4b59b73"
      },
      "source": [
        "y_train_precision = average_precision_score(Y_train, y_pred_train)\n",
        "y_train_acc1 = accuracy_score(Y_train.argmax(axis=1), y_pred_train.argmax(axis=1))\n",
        "print(\"Precision of the Logistic Regression model in test set: \" + str(y_test_precision) + \"\\n Accuracy of the Logistic Regression Modelin test set: \" + str(y_test_acc1))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision of the Logistic Regression model in test set: 0.25005542865124253\n",
            " Accuracy of the Logistic Regression Modelin test set: 0.6490993995997332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmovHnTkkmo8",
        "colab_type": "code",
        "outputId": "cdebe2f4-2a2d-469e-91de-40c50878bf8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "# Set CNN model\n",
        "# Our system of layers => [[Conv2D -> relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
        "input_shape = (32, 32, 3)\n",
        "num_classes = 7\n",
        "\n",
        "\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Conv2D(10, kernel_size = (3, 3), activation = 'relu', padding = 'Same', input_shape = input_shape))\n",
        "model_cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model_cnn.add(Dropout(0.25))\n",
        "model_cnn.add(Conv2D(20, kernel_size = (5, 5), activation = 'relu', padding = 'Same'))\n",
        "model_cnn.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "model_cnn.add(Dropout(0.25))\n",
        "model_cnn.add(Conv2D(30, kernel_size = (7, 7), activation = 'relu', padding = 'Same'))\n",
        "model_cnn.add(MaxPooling2D(pool_size = (4, 4)))\n",
        "model_cnn.add(Dropout(0.25))\n",
        "\n",
        "model_cnn.add(Flatten())\n",
        "model_cnn.add(Dense(120, activation='relu'))\n",
        "model_cnn.add(Dropout(0.2))\n",
        "model_cnn.add(Dense(64, activation ='relu'))\n",
        "model_cnn.add(Dropout(0.2))\n",
        "model_cnn.add(Dense(num_classes, activation='softmax'))\n",
        "model_cnn.summary()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 32, 32, 10)        280       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 16, 16, 10)        0         \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 16, 16, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 16, 16, 20)        5020      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 5, 5, 20)          0         \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 5, 5, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 5, 5, 30)          29430     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 1, 1, 30)          0         \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 1, 1, 30)          0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 120)               3720      \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 120)               0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 64)                7744      \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 7)                 455       \n",
            "=================================================================\n",
            "Total params: 46,649\n",
            "Trainable params: 46,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH2ojrsEnhAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmfyozuJXP_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train3D = np.reshape(X_train, (X_train.shape[0], 32, 32, 3))\n",
        "X_test3D = np.reshape(X_test, (X_test.shape[0], 32, 32, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D72wRusZV1tM",
        "colab_type": "code",
        "outputId": "eed36e03-bec4-4b81-cc39-6573ab7427b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_cnn.fit(X_train3D, Y_train, batch_size = 32, epochs = 200)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.9855 - accuracy: 0.6658\n",
            "Epoch 2/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.9619 - accuracy: 0.6647\n",
            "Epoch 3/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.9368 - accuracy: 0.6687\n",
            "Epoch 4/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.9289 - accuracy: 0.6668\n",
            "Epoch 5/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.9283 - accuracy: 0.6715\n",
            "Epoch 6/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.9160 - accuracy: 0.6774\n",
            "Epoch 7/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.9185 - accuracy: 0.6807\n",
            "Epoch 8/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8951 - accuracy: 0.6865\n",
            "Epoch 9/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8898 - accuracy: 0.6842\n",
            "Epoch 10/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8777 - accuracy: 0.6848\n",
            "Epoch 11/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8779 - accuracy: 0.6899\n",
            "Epoch 12/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8703 - accuracy: 0.6895\n",
            "Epoch 13/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8660 - accuracy: 0.6892\n",
            "Epoch 14/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8600 - accuracy: 0.6885\n",
            "Epoch 15/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8483 - accuracy: 0.6923\n",
            "Epoch 16/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8366 - accuracy: 0.6996\n",
            "Epoch 17/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8368 - accuracy: 0.6950\n",
            "Epoch 18/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8416 - accuracy: 0.6945\n",
            "Epoch 19/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8307 - accuracy: 0.6966\n",
            "Epoch 20/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8240 - accuracy: 0.7020\n",
            "Epoch 21/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8207 - accuracy: 0.6984\n",
            "Epoch 22/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8092 - accuracy: 0.7031\n",
            "Epoch 23/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8142 - accuracy: 0.7008\n",
            "Epoch 24/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8039 - accuracy: 0.7027\n",
            "Epoch 25/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.8009 - accuracy: 0.7026\n",
            "Epoch 26/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7985 - accuracy: 0.7079\n",
            "Epoch 27/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7934 - accuracy: 0.7044\n",
            "Epoch 28/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7857 - accuracy: 0.7124\n",
            "Epoch 29/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7887 - accuracy: 0.7140\n",
            "Epoch 30/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7860 - accuracy: 0.7112\n",
            "Epoch 31/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7799 - accuracy: 0.7116\n",
            "Epoch 32/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7816 - accuracy: 0.7132\n",
            "Epoch 33/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7699 - accuracy: 0.7111\n",
            "Epoch 34/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7683 - accuracy: 0.7154\n",
            "Epoch 35/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7679 - accuracy: 0.7166\n",
            "Epoch 36/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7662 - accuracy: 0.7152\n",
            "Epoch 37/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7680 - accuracy: 0.7157\n",
            "Epoch 38/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7592 - accuracy: 0.7240\n",
            "Epoch 39/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7458 - accuracy: 0.7295\n",
            "Epoch 40/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7496 - accuracy: 0.7232\n",
            "Epoch 41/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7553 - accuracy: 0.7198\n",
            "Epoch 42/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7551 - accuracy: 0.7198\n",
            "Epoch 43/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7428 - accuracy: 0.7249\n",
            "Epoch 44/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7476 - accuracy: 0.7284\n",
            "Epoch 45/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7449 - accuracy: 0.7276\n",
            "Epoch 46/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7554 - accuracy: 0.7172\n",
            "Epoch 47/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7326 - accuracy: 0.7291\n",
            "Epoch 48/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7364 - accuracy: 0.7266\n",
            "Epoch 49/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7307 - accuracy: 0.7317\n",
            "Epoch 50/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7413 - accuracy: 0.7289\n",
            "Epoch 51/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7351 - accuracy: 0.7309\n",
            "Epoch 52/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7296 - accuracy: 0.7298\n",
            "Epoch 53/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7247 - accuracy: 0.7323\n",
            "Epoch 54/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7180 - accuracy: 0.7342\n",
            "Epoch 55/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7328 - accuracy: 0.7266\n",
            "Epoch 56/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7260 - accuracy: 0.7376\n",
            "Epoch 57/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7288 - accuracy: 0.7331\n",
            "Epoch 58/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7256 - accuracy: 0.7337\n",
            "Epoch 59/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7194 - accuracy: 0.7358\n",
            "Epoch 60/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7075 - accuracy: 0.7410\n",
            "Epoch 61/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7103 - accuracy: 0.7396\n",
            "Epoch 62/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7105 - accuracy: 0.7390\n",
            "Epoch 63/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7262 - accuracy: 0.7313\n",
            "Epoch 64/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7179 - accuracy: 0.7316\n",
            "Epoch 65/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7049 - accuracy: 0.7395\n",
            "Epoch 66/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7166 - accuracy: 0.7358\n",
            "Epoch 67/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7154 - accuracy: 0.7383\n",
            "Epoch 68/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7062 - accuracy: 0.7411\n",
            "Epoch 69/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6960 - accuracy: 0.7428\n",
            "Epoch 70/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7021 - accuracy: 0.7425\n",
            "Epoch 71/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7016 - accuracy: 0.7397\n",
            "Epoch 72/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7063 - accuracy: 0.7386\n",
            "Epoch 73/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.7010 - accuracy: 0.7398\n",
            "Epoch 74/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6978 - accuracy: 0.7391\n",
            "Epoch 75/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6900 - accuracy: 0.7451\n",
            "Epoch 76/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6947 - accuracy: 0.7422\n",
            "Epoch 77/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6921 - accuracy: 0.7418\n",
            "Epoch 78/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6817 - accuracy: 0.7444\n",
            "Epoch 79/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6862 - accuracy: 0.7477\n",
            "Epoch 80/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6887 - accuracy: 0.7450\n",
            "Epoch 81/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6880 - accuracy: 0.7484\n",
            "Epoch 82/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6976 - accuracy: 0.7463\n",
            "Epoch 83/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6853 - accuracy: 0.7432\n",
            "Epoch 84/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6890 - accuracy: 0.7463\n",
            "Epoch 85/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6819 - accuracy: 0.7535\n",
            "Epoch 86/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6863 - accuracy: 0.7459\n",
            "Epoch 87/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6913 - accuracy: 0.7466\n",
            "Epoch 88/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6840 - accuracy: 0.7481\n",
            "Epoch 89/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6883 - accuracy: 0.7508\n",
            "Epoch 90/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6874 - accuracy: 0.7446\n",
            "Epoch 91/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6745 - accuracy: 0.7550\n",
            "Epoch 92/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6783 - accuracy: 0.7524\n",
            "Epoch 93/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6741 - accuracy: 0.7539\n",
            "Epoch 94/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6875 - accuracy: 0.7466\n",
            "Epoch 95/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6726 - accuracy: 0.7542\n",
            "Epoch 96/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6746 - accuracy: 0.7505\n",
            "Epoch 97/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6812 - accuracy: 0.7479\n",
            "Epoch 98/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6734 - accuracy: 0.7491\n",
            "Epoch 99/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6682 - accuracy: 0.7522\n",
            "Epoch 100/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6717 - accuracy: 0.7528\n",
            "Epoch 101/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6585 - accuracy: 0.7538\n",
            "Epoch 102/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6607 - accuracy: 0.7554\n",
            "Epoch 103/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6645 - accuracy: 0.7558\n",
            "Epoch 104/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6687 - accuracy: 0.7512\n",
            "Epoch 105/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6633 - accuracy: 0.7565\n",
            "Epoch 106/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6669 - accuracy: 0.7522\n",
            "Epoch 107/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6622 - accuracy: 0.7501\n",
            "Epoch 108/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6635 - accuracy: 0.7547\n",
            "Epoch 109/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6566 - accuracy: 0.7542\n",
            "Epoch 110/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6626 - accuracy: 0.7524\n",
            "Epoch 111/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6653 - accuracy: 0.7517\n",
            "Epoch 112/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6624 - accuracy: 0.7550\n",
            "Epoch 113/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6617 - accuracy: 0.7571\n",
            "Epoch 114/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6455 - accuracy: 0.7589\n",
            "Epoch 115/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6567 - accuracy: 0.7581\n",
            "Epoch 116/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6476 - accuracy: 0.7608\n",
            "Epoch 117/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6555 - accuracy: 0.7552\n",
            "Epoch 118/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6619 - accuracy: 0.7579\n",
            "Epoch 119/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6423 - accuracy: 0.7630\n",
            "Epoch 120/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6592 - accuracy: 0.7611\n",
            "Epoch 121/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6506 - accuracy: 0.7607\n",
            "Epoch 122/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6439 - accuracy: 0.7599\n",
            "Epoch 123/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6520 - accuracy: 0.7615\n",
            "Epoch 124/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6569 - accuracy: 0.7617\n",
            "Epoch 125/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6524 - accuracy: 0.7552\n",
            "Epoch 126/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6436 - accuracy: 0.7601\n",
            "Epoch 127/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6596 - accuracy: 0.7598\n",
            "Epoch 128/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6455 - accuracy: 0.7654\n",
            "Epoch 129/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6472 - accuracy: 0.7656\n",
            "Epoch 130/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6432 - accuracy: 0.7629\n",
            "Epoch 131/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6667 - accuracy: 0.7604\n",
            "Epoch 132/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6284 - accuracy: 0.7681\n",
            "Epoch 133/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6422 - accuracy: 0.7605\n",
            "Epoch 134/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6448 - accuracy: 0.7628\n",
            "Epoch 135/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6327 - accuracy: 0.7631\n",
            "Epoch 136/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6366 - accuracy: 0.7665\n",
            "Epoch 137/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6511 - accuracy: 0.7609\n",
            "Epoch 138/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6290 - accuracy: 0.7621\n",
            "Epoch 139/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6536 - accuracy: 0.7612\n",
            "Epoch 140/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6346 - accuracy: 0.7660\n",
            "Epoch 141/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6376 - accuracy: 0.7612\n",
            "Epoch 142/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6344 - accuracy: 0.7663\n",
            "Epoch 143/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6378 - accuracy: 0.7610\n",
            "Epoch 144/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6368 - accuracy: 0.7615\n",
            "Epoch 145/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6372 - accuracy: 0.7643\n",
            "Epoch 146/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6313 - accuracy: 0.7670\n",
            "Epoch 147/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6286 - accuracy: 0.7667\n",
            "Epoch 148/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6279 - accuracy: 0.7657\n",
            "Epoch 149/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6313 - accuracy: 0.7662\n",
            "Epoch 150/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6351 - accuracy: 0.7615\n",
            "Epoch 151/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6345 - accuracy: 0.7665\n",
            "Epoch 152/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6198 - accuracy: 0.7700\n",
            "Epoch 153/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6259 - accuracy: 0.7629\n",
            "Epoch 154/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6322 - accuracy: 0.7668\n",
            "Epoch 155/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6153 - accuracy: 0.7718\n",
            "Epoch 156/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6355 - accuracy: 0.7657\n",
            "Epoch 157/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6292 - accuracy: 0.7713\n",
            "Epoch 158/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6212 - accuracy: 0.7660\n",
            "Epoch 159/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6328 - accuracy: 0.7689\n",
            "Epoch 160/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6371 - accuracy: 0.7660\n",
            "Epoch 161/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6319 - accuracy: 0.7636\n",
            "Epoch 162/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6302 - accuracy: 0.7685\n",
            "Epoch 163/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6370 - accuracy: 0.7630\n",
            "Epoch 164/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6231 - accuracy: 0.7723\n",
            "Epoch 165/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6150 - accuracy: 0.7735\n",
            "Epoch 166/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6216 - accuracy: 0.7695\n",
            "Epoch 167/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6242 - accuracy: 0.7711\n",
            "Epoch 168/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6247 - accuracy: 0.7721\n",
            "Epoch 169/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6171 - accuracy: 0.7743\n",
            "Epoch 170/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6416 - accuracy: 0.7642\n",
            "Epoch 171/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6421 - accuracy: 0.7607\n",
            "Epoch 172/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6180 - accuracy: 0.7708\n",
            "Epoch 173/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6069 - accuracy: 0.7741\n",
            "Epoch 174/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6246 - accuracy: 0.7690\n",
            "Epoch 175/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6199 - accuracy: 0.7723\n",
            "Epoch 176/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6183 - accuracy: 0.7718\n",
            "Epoch 177/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6203 - accuracy: 0.7730\n",
            "Epoch 178/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6224 - accuracy: 0.7717\n",
            "Epoch 179/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6196 - accuracy: 0.7737\n",
            "Epoch 180/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6166 - accuracy: 0.7687\n",
            "Epoch 181/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6233 - accuracy: 0.7717\n",
            "Epoch 182/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6110 - accuracy: 0.7775\n",
            "Epoch 183/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6166 - accuracy: 0.7741\n",
            "Epoch 184/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6089 - accuracy: 0.7753\n",
            "Epoch 185/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6220 - accuracy: 0.7713\n",
            "Epoch 186/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6222 - accuracy: 0.7750\n",
            "Epoch 187/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6110 - accuracy: 0.7749\n",
            "Epoch 188/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6113 - accuracy: 0.7741\n",
            "Epoch 189/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6127 - accuracy: 0.7696\n",
            "Epoch 190/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6065 - accuracy: 0.7748\n",
            "Epoch 191/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6038 - accuracy: 0.7737\n",
            "Epoch 192/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6172 - accuracy: 0.7713\n",
            "Epoch 193/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6156 - accuracy: 0.7747\n",
            "Epoch 194/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6087 - accuracy: 0.7738\n",
            "Epoch 195/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6078 - accuracy: 0.7742\n",
            "Epoch 196/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6171 - accuracy: 0.7734\n",
            "Epoch 197/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6123 - accuracy: 0.7723\n",
            "Epoch 198/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6179 - accuracy: 0.7716\n",
            "Epoch 199/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6053 - accuracy: 0.7728\n",
            "Epoch 200/200\n",
            "8494/8494 [==============================] - 11s 1ms/step - loss: 0.6093 - accuracy: 0.7756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbb495ded30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMce7hVaWCF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3351f6c0-29d3-4294-8d2e-64d9508e0f53"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, average_precision_score, f1_score\n",
        "y_pred_test = model_cnn.predict(X_test3D)\n",
        "y_pred_train = model_cnn.predict(X_train3D)\n",
        "conf_m_test = confusion_matrix(Y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
        "y_test_precision = average_precision_score(Y_test, y_pred_test)\n",
        "y_test_acc1 = accuracy_score(Y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
        "y_test_f1 = f1_score(Y_test.argmax(axis=1), y_pred_test.argmax(axis=1), average = 'weighted')\n",
        "y_train_f1 = f1_score(Y_train.argmax(axis=1), y_pred_train.argmax(axis=1), average = 'weighted')\n",
        "print(\"Precision of the Logistic Regression model in test set: \" + str(y_test_precision) + \"\\n Accuracy of the Logistic Regression Modelin test set: \" + str(y_test_acc1))\n",
        "print(\"f1 of the Logistic Regression model in test set: \" + str(y_test_f1))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision of the Logistic Regression model in test set: 0.3878177126603025\n",
            " Accuracy of the Logistic Regression Modelin test set: 0.7084723148765844\n",
            "f1 of the Logistic Regression model in test set: 0.6871472371918573\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
